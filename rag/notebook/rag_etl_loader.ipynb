{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ETL Pipeline –¥–ª—è Yandex Handbook\n",
    "\n",
    "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "- **Child chunks** (450 —Ç–æ–∫–µ–Ω–æ–≤) ‚Üí Qdrant –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (HYBRID: dense + sparse)\n",
    "- **Parent chunks** (1500 —Ç–æ–∫–µ–Ω–æ–≤) ‚Üí Redis –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "- **ParentDocumentRetriever** —Å–≤—è–∑—ã–≤–∞–µ—Ç chunks\n",
    "\n",
    "**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–æ–≤:**\n",
    "```bash\n",
    "uv add langchain-core langchain-community langchain-openai langchain-text-splitters langchain-qdrant\n",
    "uv add qdrant-client python-dotenv tiktoken tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "   ‚Ä¢ OpenAI API: ‚úì\n",
      "   ‚Ä¢ Embedding Model: text-embedding-3-large\n",
      "   ‚Ä¢ Qdrant URL: http://localhost:6333\n",
      "   ‚Ä¢ Redis URL: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai_api_base = os.getenv('OPENAI_API_BASE')\n",
    "embedding_model_name = os.getenv('OPENAI_EVBEDDING_MODEL_NAME', 'text-embedding-3-large')\n",
    "\n",
    "# Qdrant –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "qdrant_url = os.getenv('QDRANT_URL', 'http://localhost:6333')\n",
    "\n",
    "# Redis –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')\n",
    "\n",
    "print(\" –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"   ‚Ä¢ OpenAI API: {'‚úì' if openai_api_key else '‚úó'}\")\n",
    "print(f\"   ‚Ä¢ Embedding Model: {embedding_model_name}\")\n",
    "print(f\"   ‚Ä¢ Qdrant URL: {qdrant_url}\")\n",
    "print(f\"   ‚Ä¢ Redis URL: {redis_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ò–º–ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm-dev/project/lifelong_learning_assistant/rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "# LangChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.load import dumpd, loads\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "from langchain_community.storage import RedisStore\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams, SparseIndexParams\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\" –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –§—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_text(text: str) -> str:\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ Markdown-—Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å—Å—ã–ª–æ–∫, —Ñ–∞–π–ª–æ–≤, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ [—Ç–µ–∫—Å—Ç](url) ‚Üí —Ç–µ–∫—Å—Ç\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ![alt](src)\n",
    "    text = re.sub(r'!\\[[^\\]]*\\]\\([^)]+\\)', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ URL\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è **—Ç–µ–∫—Å—Ç** ‚Üí —Ç–µ–∫—Å—Ç\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Lowercase –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    return text.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_parent_id(meta: Dict[str, Any], text: str) -> str:\n",
    "    \"\"\"–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π parent_id\"\"\"\n",
    "    base = (\n",
    "        str(meta.get(\"source\", \"\")) + \"|\" +\n",
    "        str(meta.get(\"H1\", \"\")) + \"|\" + \n",
    "        str(meta.get(\"H2\", \"\")) + \"|\" + \n",
    "        str(meta.get(\"H3\", \"\")) + \"|\" +\n",
    "        (text[:128])\n",
    "    ).encode(\"utf-8\")\n",
    "    return hashlib.md5(base).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ë–ï–ó –æ—á–∏—Å—Ç–∫–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑: /home/llm-dev/project/lifelong_learning_assistant/data/yandex-handbook-downloaded\n",
      " –ù–∞–π–¥–µ–Ω–æ 68 .md —Ñ–∞–π–ª–æ–≤\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 68 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
      "\n",
      " –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: 9532 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: 873 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: 27611 —Ç–æ–∫–µ–Ω–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_len = lambda text: len(encoder.encode(text))\n",
    "\n",
    "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "DATA_PATH = \"/home/llm-dev/project/lifelong_learning_assistant/data/yandex-handbook-downloaded\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ë–ï–ó –æ—á–∏—Å—Ç–∫–∏ (–Ω—É–∂–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##, ###)\n",
    "print(f\" –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑: {DATA_PATH}\")\n",
    "md_files = list(Path(DATA_PATH).glob(\"*.md\"))\n",
    "print(f\" –ù–∞–π–¥–µ–Ω–æ {len(md_files)} .md —Ñ–∞–π–ª–æ–≤\")\n",
    "\n",
    "documents = []\n",
    "for file_path in md_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            'source': str(file_path),\n",
    "            'filename': file_path.name,\n",
    "            'total_tokens': tiktoken_len(content)\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "if documents:\n",
    "    token_counts = [doc.metadata['total_tokens'] for doc in documents]\n",
    "    print(f\"\\n –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {sum(token_counts)/len(token_counts):.0f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    print(f\"   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {min(token_counts)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    print(f\"   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: {max(token_counts)} —Ç–æ–∫–µ–Ω–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –ó–∞–≥—Ä—É–∑–∫–∞ index.json –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ó–∞–≥—Ä—É–∂–µ–Ω–æ 68 –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
      "\n",
      " –ü—Ä–∏–º–µ—Ä—ã:\n",
      "   ‚Ä¢ about.md ‚Üí –ù–∞—á–∞—Ç—å —É—á–∏—Ç—å—Å—è\n",
      "   ‚Ä¢ pervie-shagi.md ‚Üí –ü–µ—Ä–≤—ã–µ —à–∞–≥–∏\n",
      "   ‚Ä¢ mashinnoye-obucheniye.md ‚Üí –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º index.json\n",
    "INDEX_PATH = \"/home/llm-dev/project/lifelong_learning_assistant/data/yandex-handbook-downloaded/index.json\"\n",
    "\n",
    "with open(INDEX_PATH, 'r', encoding='utf-8') as f:\n",
    "    index_data = json.load(f)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ filename -> title\n",
    "filename_to_title = {}\n",
    "for doc_id, doc_info in index_data.items():\n",
    "    filename = doc_id + '.md'\n",
    "    title = doc_info.get('title', '')\n",
    "    filename_to_title[filename] = title\n",
    "\n",
    "print(f\" –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filename_to_title)} –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "print(f\"\\n –ü—Ä–∏–º–µ—Ä—ã:\")\n",
    "for i, (fn, title) in enumerate(list(filename_to_title.items())[:3]):\n",
    "    print(f\"   ‚Ä¢ {fn} ‚Üí {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–ª–∏—Ç—Ç–µ—Ä–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown Header Splitter –¥–ª—è parent chunks (–∏—Å–ø–æ–ª—å–∑—É–µ–º H1, H2, H3 –∫–∞–∫ –≤ clean_wiki_docs)\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"H1\"),\n",
    "    (\"##\", \"H2\"),\n",
    "    (\"###\", \"H3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "# Recursive splitter –¥–ª—è –±–æ–ª—å—à–∏—Ö parent chunks\n",
    "parent_recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# Child splitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=70,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –°–æ–∑–¥–∞–Ω–∏–µ parent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –°–æ–∑–¥–∞–Ω–æ 923 parent chunks\n",
      "\n",
      " –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ parent chunks:\n",
      "   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: 668 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: 21 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: 1500 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: 584 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "\n",
      " –≠—Ñ—Ñ–µ–∫—Ç –æ—á–∏—Å—Ç–∫–∏:\n",
      "   ‚Ä¢ –î–æ –æ—á–∏—Å—Ç–∫–∏: 653,624 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: 595,848 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: 8.8%\n",
      "\n",
      " –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\n",
      "   ‚Ä¢ –ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: 50 chunks (5.4%)\n",
      "   ‚Ä¢ H2: 401 chunks (43.4%)\n",
      "   ‚Ä¢ H3: 472 chunks (51.1%)\n",
      "\n",
      " –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\n",
      "   ‚Ä¢ doc_title: –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—é\n",
      "   ‚Ä¢ path: –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—é / –°–æ–ø—Ä—è–∂—ë–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
      "   ‚Ä¢ breadcrumbs: –°–æ–ø—Ä—è–∂—ë–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
      "   ‚Ä¢ level: 2\n"
     ]
    }
   ],
   "source": [
    "parent_docs = []\n",
    "total_before_clean = 0\n",
    "total_after_clean = 0\n",
    "\n",
    "for doc in documents:\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ index.json\n",
    "    doc_filename = doc.metadata.get('filename', '')\n",
    "    doc_title = filename_to_title.get(doc_filename, '')\n",
    "    \n",
    "    # 1. –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º (–î–û –æ—á–∏—Å—Ç–∫–∏!)\n",
    "    md_chunks = markdown_splitter.split_text(doc.page_content or \"\")\n",
    "    \n",
    "    for chunk in md_chunks:\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "        merged_meta = {**(doc.metadata or {}), **(chunk.metadata or {})}\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        merged_meta[\"doc_title\"] = doc_title\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º breadcrumbs –∏ path (–∫–∞–∫ –≤ clean_wiki_docs)\n",
    "        headers = [merged_meta.get(k, \"\") for k in (\"H1\", \"H2\", \"H3\") if k in merged_meta]\n",
    "        merged_meta[\"breadcrumbs\"] = \" / \".join(headers) if headers else \"\"\n",
    "        \n",
    "        # Path: doc_title / H1 / H2 / H3\n",
    "        path_parts = [doc_title] if doc_title else []\n",
    "        path_parts.extend(headers)\n",
    "        merged_meta[\"path\"] = \" / \".join(path_parts) if path_parts else \"\"\n",
    "        \n",
    "        # Level: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –Ω–∞–ª–∏—á–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        merged_meta[\"level\"] = 3 if \"H3\" in merged_meta else 2 if \"H2\" in merged_meta else 1 if \"H1\" in merged_meta else 0\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ –æ—á–∏—Å—Ç–∫–∏\n",
    "        total_before_clean += tiktoken_len(chunk.page_content)\n",
    "        \n",
    "        # 2. –¢–ï–ü–ï–†–¨ –æ—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç chunk (lowercase –∏ —É–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞)\n",
    "        chunk.page_content = clean_document_text(chunk.page_content)\n",
    "        total_after_clean += tiktoken_len(chunk.page_content)\n",
    "        \n",
    "        # 3. –°–æ–∑–¥–∞–µ–º parent_id\n",
    "        merged_meta[\"parent_id\"] = stable_parent_id(merged_meta, chunk.page_content)\n",
    "        chunk.metadata = merged_meta\n",
    "        \n",
    "        # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - —Ä–∞–∑–±–∏–≤–∞–µ–º\n",
    "        if tiktoken_len(chunk.page_content) > 1500:\n",
    "            sub_chunks = parent_recursive_splitter.split_documents([chunk])\n",
    "            for sub in sub_chunks:\n",
    "                sub.metadata = merged_meta.copy()\n",
    "                sub.metadata[\"parent_id\"] = stable_parent_id(sub.metadata, sub.page_content)\n",
    "            parent_docs.extend(sub_chunks)\n",
    "        else:\n",
    "            parent_docs.append(chunk)\n",
    "\n",
    "print(f\" –°–æ–∑–¥–∞–Ω–æ {len(parent_docs)} parent chunks\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "parent_tokens = [tiktoken_len(doc.page_content) for doc in parent_docs]\n",
    "levels = [doc.metadata.get('level', 0) for doc in parent_docs]\n",
    "\n",
    "print(f\"\\n –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ parent chunks:\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {sum(parent_tokens)/len(parent_tokens):.0f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {min(parent_tokens)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: {max(parent_tokens)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {sorted(parent_tokens)[len(parent_tokens)//2]} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "print(f\"\\n –≠—Ñ—Ñ–µ–∫—Ç –æ—á–∏—Å—Ç–∫–∏:\")\n",
    "print(f\"   ‚Ä¢ –î–æ –æ—á–∏—Å—Ç–∫–∏: {total_before_clean:,} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {total_after_clean:,} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: {(1 - total_after_clean/total_before_clean)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\")\n",
    "for i in range(4):\n",
    "    count = levels.count(i)\n",
    "    if count > 0:\n",
    "        level_name = [\"–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞\", \"H1\", \"H2\", \"H3\"][i]\n",
    "        print(f\"   ‚Ä¢ {level_name}: {count} chunks ({count/len(parent_docs)*100:.1f}%)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
    "print(f\"\\n –ü—Ä–∏–º–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:\")\n",
    "if parent_docs:\n",
    "    sample = parent_docs[10]  # –ë–µ—Ä–µ–º 10-–π –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n",
    "    print(f\"   ‚Ä¢ doc_title: {sample.metadata.get('doc_title', '–Ω–µ—Ç')}\")\n",
    "    print(f\"   ‚Ä¢ path: {sample.metadata.get('path', '–Ω–µ—Ç')}\")\n",
    "    print(f\"   ‚Ä¢ breadcrumbs: {sample.metadata.get('breadcrumbs', '–Ω–µ—Ç')}\")\n",
    "    print(f\"   ‚Ä¢ level: {sample.metadata.get('level', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/home/llm-dev/project/lifelong_learning_assistant/data/yandex-handbook-downloaded/veroyatnostnyj-podhod-v-ml.md',\n",
       " 'filename': 'veroyatnostnyj-podhod-v-ml.md',\n",
       " 'total_tokens': 6115,\n",
       " 'doc_title': '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ ML',\n",
       " 'breadcrumbs': '',\n",
       " 'path': '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ ML',\n",
       " 'level': 0,\n",
       " 'parent_id': '43a09260959c8673a66e2810d418de54'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Qdrant –∏ Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Fetching 18 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (dense + sparse)\n",
      "üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è\n",
      " Qdrant –Ω–∞—Å—Ç—Ä–æ–µ–Ω (HYBRID —Ä–µ–∂–∏–º): yandex_handbook_child_chunks\n",
      " Redis –Ω–∞—Å—Ç—Ä–æ–µ–Ω: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "# Dense embeddings (OpenAI)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=embedding_model_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    chunk_size=1000\n",
    ")\n",
    "\n",
    "# Sparse embeddings (BM25 –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "print(\" –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (dense + sparse)\")\n",
    "\n",
    "# Qdrant\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"yandex_handbook_child_chunks\"\n",
    "\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"dense\": VectorParams(size=3072, distance=Distance.COSINE)\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": SparseVectorParams(\n",
    "            index=SparseIndexParams(on_disk=False),\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–º —Ä–µ–∂–∏–º–µ\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    vector_name=\"dense\",\n",
    "    sparse_vector_name=\"sparse\",\n",
    ")\n",
    "\n",
    "print(f\" Qdrant –Ω–∞—Å—Ç—Ä–æ–µ–Ω (HYBRID —Ä–µ–∂–∏–º): {collection_name}\")\n",
    "\n",
    "# Redis\n",
    "parent_store = RedisStore(\n",
    "    redis_url=redis_url,\n",
    "    namespace=\"rag:parents\"\n",
    ")\n",
    "\n",
    "print(f\" Redis –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {redis_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant –∏ Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\n",
      " –ó–∞–≥—Ä—É–∂–µ–Ω–æ 923 parent chunks –≤ Redis\n"
     ]
    }
   ],
   "source": [
    "print(\" –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Redis\n",
    "def doc_to_bytes(doc: Document) -> bytes:\n",
    "    as_dict = dumpd(doc)\n",
    "    return json.dumps(as_dict, ensure_ascii=False).encode(\"utf-8\")\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º parent chunks –≤ Redis\n",
    "parent_ids = []\n",
    "batch = []\n",
    "for doc in parent_docs:\n",
    "    pid = doc.metadata[\"parent_id\"]\n",
    "    batch.append((pid, doc_to_bytes(doc)))\n",
    "    parent_ids.append(pid)\n",
    "    \n",
    "    if len(batch) >= 1000:\n",
    "        parent_store.mset(batch)\n",
    "        batch.clear()\n",
    "\n",
    "if batch:\n",
    "    parent_store.mset(batch)\n",
    "\n",
    "print(f\" –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(parent_ids)} parent chunks –≤ Redis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –°–æ–∑–¥–∞–Ω–æ 2071 child chunks\n",
      "   ‚Ä¢ Path –¥–æ–±–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–æ –∫–∞–∂–¥–æ–≥–æ child chunk –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
      "\n",
      " –ó–∞–≥—Ä—É–∂–∞–µ–º 2071 child chunks –≤ Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:44<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Child chunks –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Qdrant\n",
      "\n",
      " –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: 68\n",
      "   ‚Ä¢ Parent chunks: 923\n",
      "   ‚Ä¢ Child chunks: 2071\n",
      "   ‚Ä¢ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ child/parent: 2.2\n",
      "\n",
      " Qdrant: 2,071 –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
      " Redis: 923 parent –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
      "\n",
      " –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ child chunks:\n",
      "   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: 355 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: 20 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: 515 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: 396 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "\n",
      " –ü—Ä–∏–º–µ—Ä child chunk:\n",
      "   ‚Ä¢ –ü—Ä–µ–≤—å—é: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ ML / –°–ª—É—á–∞–π–Ω–æ—Å—Ç—å –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏  —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–µ–π —Å –∫–∞–∂–¥—ã–º –∏–∑ —Ç–∏–ø–æ–≤ —à—É–º–∞: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º, –ª–∞–ø–ª–∞—Å–æ–≤—Å–∫–∏–º –∏ –∫–æ—à–∏. !9 –∫–∞–∫ –≤—ã –º–æ–≥–ª–∏ –∑–∞–º–µ—Ç–∏—Ç—å, –≤ –∫–∞–∂–¥–æ–º –∏–∑ –ø–æ–¥—Ö–æ–¥–æ–≤ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–æ –µ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã ), –æ—Å—Ç–∞—ë—Ç—Å—è —Å–≤–æ—è —Å—Ç–µ–ø–µ–Ω—å —Å–≤–æ–±–æ–¥...\n"
     ]
    }
   ],
   "source": [
    "# 2. –°–æ–∑–¥–∞–µ–º child chunks –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ Qdrant\n",
    "child_docs = []\n",
    "for parent in parent_docs:\n",
    "    parent_id = parent.metadata[\"parent_id\"]\n",
    "    children = child_splitter.split_documents([parent])\n",
    "    \n",
    "    for child in children:\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "        child.metadata.update(parent.metadata)\n",
    "        child.metadata[\"parent_id\"] = parent_id\n",
    "        child.metadata[\"child_id\"] = str(uuid4())\n",
    "        child.metadata[\"is_child_chunk\"] = True\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º path –≤ –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "        path = child.metadata.get(\"path\", \"\")\n",
    "        if path:\n",
    "            child.page_content = f\"{path}\\n\\n{child.page_content}\"\n",
    "        \n",
    "        child_docs.append(child)\n",
    "\n",
    "print(f\" –°–æ–∑–¥–∞–Ω–æ {len(child_docs)} child chunks\")\n",
    "print(f\"   ‚Ä¢ Path –¥–æ–±–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–æ –∫–∞–∂–¥–æ–≥–æ child chunk –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Qdrant —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º\n",
    "if child_docs:\n",
    "    batch_size = 100\n",
    "    print(f\"\\n –ó–∞–≥—Ä—É–∂–∞–µ–º {len(child_docs)} child chunks –≤ Qdrant...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(child_docs), batch_size), desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant\"):\n",
    "        batch = child_docs[i:i+batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "    \n",
    "    print(f\" Child chunks –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Qdrant\")\n",
    "\n",
    "# –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(f\"\\n –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "print(f\"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\")\n",
    "print(f\"   ‚Ä¢ Parent chunks: {len(parent_docs)}\")\n",
    "print(f\"   ‚Ä¢ Child chunks: {len(child_docs)}\")\n",
    "print(f\"   ‚Ä¢ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ child/parent: {len(child_docs)/len(parent_docs):.1f}\")\n",
    "\n",
    "collection_info = qdrant_client.get_collection(collection_name)\n",
    "print(f\"\\n Qdrant: {collection_info.points_count:,} –≤–µ–∫—Ç–æ—Ä–æ–≤\")\n",
    "print(f\" Redis: {len(parent_ids):,} parent –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ child chunks\n",
    "child_tokens = [tiktoken_len(doc.page_content) for doc in child_docs]\n",
    "print(f\"\\n –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ child chunks:\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {sum(child_tokens)/len(child_tokens):.0f} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {min(child_tokens)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: {max(child_tokens)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {sorted(child_tokens)[len(child_tokens)//2]} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä child chunk —Å path\n",
    "print(f\"\\n –ü—Ä–∏–º–µ—Ä child chunk:\")\n",
    "if child_docs:\n",
    "    sample = child_docs[5]\n",
    "    preview = sample.page_content[:300].replace('\\n', ' ')\n",
    "    print(f\"   ‚Ä¢ –ü—Ä–µ–≤—å—é: {preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫:\n",
      "============================================================\n",
      "\n",
      "1. –ó–∞–ø—Ä–æ—Å: '–±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3085/3734101441.py:2: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return loads(b.decode(\"utf-8\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    –ù–∞–π–¥–µ–Ω–æ: 10 parent chunks\n",
      "    –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n",
      "      ‚Ä¢ –§–∞–π–ª: bajesovskij-podhod-k-ocenivaniyu.md\n",
      "      ‚Ä¢ Breadcrumbs: –ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
      "      ‚Ä¢ –†–∞–∑–º–µ—Ä: 326 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "      ‚Ä¢ –ü—Ä–µ–≤—å—é: ## –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–æ —Å–∏—Ö –ø–æ—Ä –º—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–∞–ª–∏ –æ –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫–∞–∫ –æ —á—ë–º-—Ç–æ, —á—Ç–æ –æ–¥–∏–Ω —Ä–∞–∑ –æ–±—É—á–∞–µ—Ç—Å—è –∏ –¥–∞–ª—å—à–µ –Ω–∞–≤—Å–µ–≥–¥–∞ –∑–∞—Å—Ç—ã–≤–∞–µ—Ç –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ, –Ω–æ –≤ –∂–∏–∑–Ω–∏ —Ç–∞...\n",
      "\n",
      "2. –ó–∞–ø—Ä–æ—Å: '—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏'\n",
      "    –ù–∞–π–¥–µ–Ω–æ: 10 parent chunks\n",
      "    –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n",
      "      ‚Ä¢ –§–∞–π–ª: tonkosti-obucheniya.md\n",
      "      ‚Ä¢ Breadcrumbs: –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
      "      ‚Ä¢ –†–∞–∑–º–µ—Ä: 417 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "      ‚Ä¢ –ü—Ä–µ–≤—å—é: ## —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å–º—ã—Å–ª —Ç–µ—Ä–º–∏–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (–∞–Ω–≥–ª. regularization) –≥–æ—Ä–∞–∑–¥–æ —à–∏—Ä–µ –ø—Ä–∏–≤—ã—á–Ω–æ–≥–æ –≤–∞–º –ø—Ä–∏–±–∞–≤–ª–µ–Ω–∏—è - –∏–ª–∏ -–Ω–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–∞ –≤–µ—Å–æ–≤ –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ–Ω –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–æ–ª—å—à–æ...\n",
      "\n",
      "3. –ó–∞–ø—Ä–æ—Å: '–∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤'\n",
      "    –ù–∞–π–¥–µ–Ω–æ: 10 parent chunks\n",
      "    –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\n",
      "      ‚Ä¢ –§–∞–π–ª: bajesovskij-podhod-k-ocenivaniyu.md\n",
      "      ‚Ä¢ Breadcrumbs: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
      "      ‚Ä¢ –†–∞–∑–º–µ—Ä: 959 —Ç–æ–∫–µ–Ω–æ–≤\n",
      "      ‚Ä¢ –ü—Ä–µ–≤—å—é: ## –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–≤–∞–π—Ç–µ –Ω–µ–Ω–∞–¥–æ–ª–≥–æ –∑–∞–±—É–¥–µ–º –ø—Ä–æ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º, —á—Ç–æ –º—ã –ø–æ–¥–æ–±—Ä–∞–ª–∏ —Å –ø–æ–ª–∞ –º–æ–Ω–µ—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ø–∞–¥–∞–µ—Ç –æ—Ä–ª–æ–º —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø–æ–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç...\n"
     ]
    }
   ],
   "source": [
    "def bytes_to_doc(b: bytes) -> Document:\n",
    "    return loads(b.decode(\"utf-8\"))\n",
    "\n",
    "def search(query: str, k: int = 10):\n",
    "    \"\"\"–ü–æ–∏—Å–∫ —Å Parent-Child Retrieval\"\"\"\n",
    "    # –ù–∞—Ö–æ–¥–∏–º child chunks\n",
    "    child_results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º parent chunks –∏–∑ Redis\n",
    "    parent_ids = [doc.metadata.get(\"parent_id\") for doc in child_results]\n",
    "    parent_bytes = parent_store.mget(parent_ids)\n",
    "    \n",
    "    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º parent –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "    results = []\n",
    "    for pb in parent_bytes:\n",
    "        if pb:\n",
    "            parent_doc = bytes_to_doc(pb)\n",
    "            parent_doc.metadata[\"chunk_type\"] = \"parent\"\n",
    "            results.append(parent_doc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n",
    "test_queries = [\n",
    "    \"–±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏\",\n",
    "    \"—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\",\n",
    "    \"–∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\"\n",
    "]\n",
    "\n",
    "print(\"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫:\\n\" + \"=\"*60)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{i}. –ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
    "    results = search(query)\n",
    "    \n",
    "    if results:\n",
    "        best = results[0]\n",
    "        print(f\"    –ù–∞–π–¥–µ–Ω–æ: {len(results)} parent chunks\")\n",
    "        print(f\"    –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:\")\n",
    "        print(f\"      ‚Ä¢ –§–∞–π–ª: {best.metadata.get('filename', 'unknown')}\")\n",
    "        print(f\"      ‚Ä¢ Breadcrumbs: {best.metadata.get('breadcrumbs', '–Ω–µ—Ç')}\")\n",
    "        print(f\"      ‚Ä¢ –†–∞–∑–º–µ—Ä: {tiktoken_len(best.page_content)} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "        preview = best.page_content[:200].replace('\\n', ' ')\n",
    "        print(f\"      ‚Ä¢ –ü—Ä–µ–≤—å—é: {preview}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
